#!/usr/bin/env python3
# coding: utf-8
"""
T·∫°o embedding cho to√†n b·ªô JD trong jds.csv b·∫±ng ƒë√∫ng pipeline embedding c·ªßa courses:
- SBERT (model 768 chi·ªÅu)
- PCA (ƒë√£ fit s·∫µn)
- scaler (n·∫øu c√≥)
- predict cluster b·∫±ng cosine v·ªõi centroids

ƒêi·ªÉm m·ªõi (auto-fix):
‚úî T·ª± ƒë·ªông ch·ªçn ƒë√∫ng c·ªôt text n·∫øu kh√¥ng truy·ªÅn
‚úî H·ªó tr·ª£ g·ªôp title + company + full_content_clean th√†nh m·ªôt chu·ªói
‚úî M·∫∑c ƒë·ªãnh d√πng ƒë√∫ng model 768-dim: all-mpnet-base-v2
‚úî Kh√¥ng c√≤n l·ªói mismatch k√≠ch th∆∞·ªõc embedding
‚úî D·ªÖ ch·∫°y, kh√¥ng ph·∫£i nh·ªõ tham s·ªë ph·ª©c t·∫°p

Output:
    out_dir/jd_emb.npy
    out_dir/jd_cluster_map.csv
"""

import os
import argparse
import numpy as np
import pandas as pd
import pickle
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity


def ensure_dir(p):
    os.makedirs(p, exist_ok=True)


def auto_detect_text_column(df, user_col=None):
    """
    T·ª± ƒë·ªông t√¨m c·ªôt ch·ª©a n·ªôi dung JD.
    ∆Øu ti√™n user_col, sau ƒë√≥ th·ª≠ full_content_clean, jd_text, description.
    """
    if user_col and user_col in df.columns:
        return user_col

    candidates = ["full_content_clean", "jd_text", "description", "text"]
    for c in candidates:
        if c in df.columns:
            return c

    raise ValueError("Kh√¥ng t√¨m th·∫•y c·ªôt ch·ª©a n·ªôi dung JD! Vui l√≤ng cung c·∫•p --text_col.")


def build_text_field(df, text_col):
    """
    Gh√©p th√™m title + company n·∫øu c√≥ ƒë·ªÉ tƒÉng ch·∫•t l∆∞·ª£ng embedding.
    """
    title = df["title"].fillna("").astype(str) if "title" in df.columns else ""
    company = df["company"].fillna("").astype(str) if "company" in df.columns else ""
    content = df[text_col].fillna("").astype(str)

    full_text = (title + " " + company + " " + content).str.strip()
    return full_text.tolist()


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--csv", required=True, help="Path to jds.csv")
    parser.add_argument("--text_col", default=None, help="T√™n c·ªôt ch·ª©a JD (optional)")
    parser.add_argument("--pca_path", required=True, help="pca_model.pkl")
    parser.add_argument("--centroid_path", required=True, help="centroids_kX.npy")
    parser.add_argument("--scaler_path", required=False, help="scaler_for_selection.pkl")
    parser.add_argument("--model_name", default=None, help="SBERT model n·∫øu mu·ªën override")
    parser.add_argument("--out_dir", default="outputs/embeddings")
    args = parser.parse_args()

    ensure_dir(args.out_dir)

    # ---------------------------------------------------------
    # Load JD CSV
    # ---------------------------------------------------------
    df = pd.read_csv(args.csv)
    text_col = auto_detect_text_column(df, args.text_col)
    print(f"Using JD text column: {text_col}")

    texts = build_text_field(df, text_col)
    print(f"Loaded {len(texts)} JD entries.")

    # ---------------------------------------------------------
    # Ch·ªçn model SBERT ƒë√∫ng (m·∫∑c ƒë·ªãnh 768-dim)
    # ---------------------------------------------------------
    DEFAULT_MODEL = "sentence-transformers/all-mpnet-base-v2"

    model_name = args.model_name if args.model_name else DEFAULT_MODEL
    print(f"Using SBERT model: {model_name}")

    model = SentenceTransformer(model_name)

    # ---------------------------------------------------------
    # Encode JD text
    # ---------------------------------------------------------
    print("Encoding JD embeddings...")
    emb = model.encode(texts, normalize_embeddings=True)
    print("Raw JD embedding shape:", emb.shape)

    # ---------------------------------------------------------
    # PCA (must match courses pipeline)
    # ---------------------------------------------------------
    pca = pickle.load(open(args.pca_path, "rb"))

    # Ki·ªÉm tra dimension kh·ªõp PCA input
    if emb.shape[1] != pca.n_features_in_:
        raise ValueError(
            f"\n‚ùå Sai model SBERT!\n"
            f"JD embedding c√≥ {emb.shape[1]} chi·ªÅu nh∆∞ng PCA y√™u c·∫ßu {pca.n_features_in_} chi·ªÅu.\n"
            f"‚Üí B·∫°n ph·∫£i d√πng ƒë√∫ng model SBERT ƒë√£ d√πng cho courses.\n"
        )

    emb_pca = pca.transform(emb)
    print("After PCA:", emb_pca.shape)

    # ---------------------------------------------------------
    # Scale (if used during clustering)
    # ---------------------------------------------------------
    if args.scaler_path and os.path.exists(args.scaler_path):
        scaler = pickle.load(open(args.scaler_path, "rb"))
        emb_scaled = scaler.transform(emb_pca)
        print("Scaler applied.")
    else:
        emb_scaled = emb_pca
        print("No scaler used.")

    # ---------------------------------------------------------
    # Load centroids + predict cluster
    # ---------------------------------------------------------
    centroids = np.load(args.centroid_path)
    sims = cosine_similarity(emb_scaled, centroids)
    cluster_ids = sims.argmax(axis=1)
    scores = sims.max(axis=1)

    # ---------------------------------------------------------
    # Save outputs
    # ---------------------------------------------------------
    np.save(os.path.join(args.out_dir, "jd_emb.npy"), emb_scaled)
    print("Saved jd_emb.npy")

    df_out = df.copy()
    df_out["cluster"] = cluster_ids
    df_out["score"] = scores
    df_out.to_csv(os.path.join(args.out_dir, "jd_cluster_map.csv"), index=False)
    print("Saved jd_cluster_map.csv")

    print("\nüéâ DONE! JD embeddings + cluster assignment generated without errors!")


if __name__ == "__main__":
    main()
